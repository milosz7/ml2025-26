{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1HIMxUl17y"
      },
      "source": [
        "# Sieci konwolucyjne\n",
        "\n",
        "Zadanie dla Państwa na te ćwiczenia to implementacja funkcji konwolucji oraz max pooling dla obrazów."
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA1sA9rnl17z",
        "outputId": "cb38d40e-1a83-4a01-ebfb-dc675e20555b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ml2025-26'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 114 (delta 38), reused 107 (delta 33), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 12.71 MiB | 25.58 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "try:\n",
        "    # one of the ways to check if our notebook is running inside google colab\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/gmum/ml2025-26.git\n",
        "    import sys\n",
        "    sys.path.append('/content/ml2025-26/lab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NnTFJtDil170"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "0J-Xn5yTl170",
        "outputId": "9054611c-46b4-43e1-cee1-0462ab013e2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x792f32a30710>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# data loading\n",
        "if IN_COLAB:\n",
        "    cifar_sample = np.load(\"/content/ml2025-26/lab/resources/cifar_sample.npy\")\n",
        "else:\n",
        "    cifar_sample = np.load(\"resources/cifar_sample.npy\")\n",
        "\n",
        "# get a first random image\n",
        "np_image = cifar_sample[0]\n",
        "# this should plot a blurry frog\n",
        "plt.imshow(np_image.transpose(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csmcY8tAl170"
      },
      "source": [
        "### Wzory na rozmiary\n",
        "**Pytanie 1**: Jaki będzie rozmiar obrazka na wyjściu konwolucji/poolingu przy parametrach poniżej.  \n",
        "**Uwaga**: zarówno we wzorach jak i w kodzie używana jest torchowa konwencja *channel first*.\n",
        "\n",
        "Stride: $ \\hspace{95px} S $  \n",
        "Padding: $ \\hspace{80px} P $  \n",
        "Obrazek wejściowy: $ \\hspace{12px} C_i \\times H_i \\times W_i$  \n",
        "Filtry: $ \\hspace{100px} K \\times C_f \\times F \\times F $  \n",
        "\n",
        "Gdzie: $C_i$ to liczba kanału obrazu wejściowego, $H_i, W_i$ to odpowiednio wysokość i szerokość obrazu wejściowego. $K$ to liczba filtrów, $C_f$ liczba kanałów w każdym filtrze, $F$ to zarówno wysokość jak i szerokość filtra (rozważamy tylko filtry kwadratowe).\n",
        "\n",
        "Obrazek wyjściowy: $ \\hspace{15px} C_o \\times H_o \\times W_o $  \n",
        "\n",
        "\n",
        "$ \\hspace{140px} C_o = K $  \n",
        "\n",
        "$ \\hspace{140px} H_o =  \\frac{H_i - F + 2P}{S} + 1  $\n",
        "\n",
        "$ \\hspace{140px} W_o = \\frac{W_i - F + 2P}{S} + 1 $  \n",
        "\n",
        "**Pytanie 2**: Ile wag (floatów) ma taka warstwa konwolucyja?   \n",
        "\n",
        "$ ( F \\times F \\times C_f + 1 ) \\times K $\n",
        "\n",
        "### Wizualna pomoc do konwolucji\n",
        "[Źródło](http://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "![cnn.gif](https://raw.githubusercontent.com/gmum/ml2025-26/main/lab/resources/cnn.gif)\n",
        "\n",
        "### Zadanie 1:  Konwolucja (5 pkt.)\n",
        "Zadaniem jest zaimplementowanie funkcji konwolucji i poolingu dla obrazka 2D. Implementacja nie musi być optymalna pod względem złożoności czasowej (tzn. można/zaleca się używać pętli).\n",
        "\n",
        "Warunkiem zaliczenia zadania jest przejście komórek testowych dla konwolucji i poolingu. W razie problemów polecam zacząć od poolingu, który jest podobny do konwolucji, ale mniej skomplikowany."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "87f8kNUkl171"
      },
      "outputs": [],
      "source": [
        "def convolution(image: torch.tensor,\n",
        "                filters: torch.tensor,\n",
        "                bias: torch.tensor,\n",
        "                stride: int = 1,\n",
        "                padding: int = 1):\n",
        "    \"\"\"\n",
        "    :param image: torch.Tensor\n",
        "        Input image of shape (C, H, W)\n",
        "    :param filters: torch.Tensor\n",
        "        Filters to use in convolution of shape (K, C, F, F)\n",
        "    :param bias: torch.Tensor\n",
        "        Bias vector of shape (K,)\n",
        "    :param stride: int\n",
        "        Stride to use in convolution\n",
        "    :param padding: int\n",
        "       Zero-padding to add on all sides of the image\n",
        "    \"\"\"\n",
        "    # get image dimensions\n",
        "    img_channels, img_height, img_width = image.shape\n",
        "    n_filters, filter_channels, filter_size, filter_size = filters.shape\n",
        "    # calculate the dimensions of the output image\n",
        "    out_height = int((img_height - filter_size + 2 * padding) / stride) + 1\n",
        "    out_width = int((img_width - filter_size + 2 * padding) / stride) + 1\n",
        "    out_channels = n_filters\n",
        "    conv_out = torch.empty((out_channels, out_height, out_width))\n",
        "    padded_image = torch.tensor(np.pad(\n",
        "    image,\n",
        "    pad_width=((0, 0), (padding, padding), (padding, padding)),\n",
        "    mode='constant',\n",
        "    constant_values=0))\n",
        "\n",
        "    for i_h in range(out_height):\n",
        "      for i_w in range(out_width):\n",
        "        h_start = stride * i_h\n",
        "        w_start = stride * i_w\n",
        "        fragment = padded_image[:, h_start:h_start+filter_size, w_start:w_start+filter_size]\n",
        "        for n, filter in enumerate(filters):\n",
        "          conv_element = torch.sum(fragment * filter)\n",
        "          conv_out[n, i_h, i_w] = conv_element\n",
        "        conv_out[:, i_h, i_w] = conv_out[:, i_h, i_w] + bias\n",
        "\n",
        "    return conv_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "unA-KA4al171"
      },
      "outputs": [],
      "source": [
        "# Convolution Test\n",
        "\n",
        "# cast the frog to tensor\n",
        "image = torch.tensor(np_image)\n",
        "# preapre parameters for testing\n",
        "paddings = [0, 1, 2, 3]\n",
        "strides = [1, 2, 3, 4]\n",
        "filters = [\n",
        "    (torch.randn((2, 3, 3, 3)), torch.randn((2))),\n",
        "    (torch.randn((2, 3, 5, 5)), torch.randn((2))),\n",
        "    (torch.randn((5, 3, 1, 1)), torch.randn((5))),\n",
        "]\n",
        "\n",
        "# test all combinations\n",
        "for (filt, bias), stride, padding in product(filters, strides, paddings):\n",
        "    # your convolution\n",
        "    out = convolution(image, filt, bias, stride=stride, padding=padding)\n",
        "    # PyTorch equivalent\n",
        "    out_torch = torch.conv2d(\n",
        "        input=image.unsqueeze(0), weight=filt, bias=bias, padding=padding, stride=stride\n",
        "    )\n",
        "    # asserts\n",
        "    assert out_torch.squeeze().shape == out.shape\n",
        "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjE2ZUcll171"
      },
      "source": [
        "### Zadanie 2. Max Pooling (2 pkt.)\n",
        "Operacja *max pooling* jest analogiczna do zwykłej konwolucji, lecz zamiast operacji mnożenia z zadanym filtrem na każdym fragmencie wejścia wykonywana jest funkcja *max*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kJc1EHbol171"
      },
      "outputs": [],
      "source": [
        "def max_pooling(image: torch.tensor,\n",
        "                kernel_size: int,\n",
        "                stride: int = 1,\n",
        "                padding: int = 1):\n",
        "    \"\"\"\n",
        "    :param image: torch.Tensor\n",
        "        Input image of shape (C, H, W)\n",
        "    :param kernel_size: int\n",
        "        Size of the square pooling kernel\n",
        "    :param stride: int\n",
        "        Stride to use in pooling\n",
        "    :param padding: int\n",
        "       Zero-padding to add on all sides of the image\n",
        "    \"\"\"\n",
        "    # get image dimensions\n",
        "    img_channels, img_height, img_width = image.shape\n",
        "    # your code here\n",
        "    out_height = int((img_height - kernel_size + 2 * padding) / stride) + 1\n",
        "    out_width = int((img_width - kernel_size + 2 * padding) / stride) + 1\n",
        "    out_channels = img_channels\n",
        "    conv_out = torch.empty((out_channels, out_height, out_width))\n",
        "    padded_image = torch.tensor(np.pad(\n",
        "    image,\n",
        "    pad_width=((0, 0), (padding, padding), (padding, padding)),\n",
        "    mode='constant',\n",
        "    constant_values=0))\n",
        "\n",
        "    for i_h in range(out_height):\n",
        "      for i_w in range(out_width):\n",
        "        h_start = stride * i_h\n",
        "        w_start = stride * i_w\n",
        "        fragment = padded_image[:, h_start:h_start+kernel_size, w_start:w_start+kernel_size]\n",
        "        conv_out[:, i_h, i_w] = torch.max(fragment.reshape(fragment.size(0), -1), dim=1).values\n",
        "\n",
        "    return conv_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0QicCHgDl171"
      },
      "outputs": [],
      "source": [
        "# cast the frog to tensor\n",
        "image = torch.tensor(np_image)\n",
        "# preapre parameters for testing\n",
        "kernel_sizes = [2, 3, 4]\n",
        "paddings = [0, 1]\n",
        "strides = [1, 2, 3, 4]\n",
        "\n",
        "# test all combinations\n",
        "for kernel_size, stride, padding in product(kernel_sizes, strides, paddings):\n",
        "    # your pooling\n",
        "    out = max_pooling(image, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    # PyTorch equivalent\n",
        "    out_torch = torch.nn.functional.max_pool2d(\n",
        "        input=image.unsqueeze(0), kernel_size=kernel_size, padding=padding, stride=stride\n",
        "    )\n",
        "    # asserts\n",
        "    assert out_torch.squeeze().shape == out.shape\n",
        "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
      ]
    },
    {
      "metadata": {
        "id": "-c-d1l0Zl171"
      },
      "cell_type": "markdown",
      "source": [
        "# Trenowanie sieci kowolucyjnych\n",
        "\n",
        "**Uwaga:** Poniższa część notebooka jest stworzona z myślą o uruchamianiu go w [Colaboratory](https://colab.research.google.com/) (link do przeglądania repozytoriów w colabie: [link](https://colab.research.google.com/github/)), oznacza to, że przygotowane zadania zakładają uczenie modelu z użyciem GPU. Na zajęciach zostanie krótko omówione używanie Colaboratory. Na własną odpowiedzialność można używać własnego komputera do rozwiązania pracy domowej."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-29T09:42:53.802230Z",
          "start_time": "2024-10-29T09:42:50.749160Z"
        },
        "id": "77WYPFiWl172"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import (RandomVerticalFlip, ToTensor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "LVk__Ejxl172"
      },
      "cell_type": "markdown",
      "source": [
        "### CIFAR10\n",
        "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to dataset zawierający 60 tysięcy kolorowych obrazków rozmiaru 32 $\\times$ 32 pikseli należących do 10 różnych klas. Poniżej przykładowe wizualizacje."
      ]
    },
    {
      "metadata": {
        "id": "Wc-AAzUUl172"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "dataset = CIFAR10(root=\".\", train=True, transform=ToTensor(), download=True)\n",
        "loader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "# get a single batch\n",
        "for x, y in loader:\n",
        "    break\n",
        "\n",
        "# prepare visualisations\n",
        "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(30, 15))\n",
        "for i, row in enumerate(axes):\n",
        "    for j, ax in enumerate(row):\n",
        "        ind = 5 * i + j\n",
        "        ax.imshow(np.transpose(x[ind], (1, 2, 0)))\n",
        "        ax.set_title(f\"{dataset.classes[y[ind]]}\", fontdict={\"size\": 30})\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "print(f\"CIFAR10 classes: {dataset.classes}\")"
      ]
    },
    {
      "metadata": {
        "id": "iduID-v_l172"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie 3. Sieć Konwolucyjna na CIFAR10 (4 pkt.)\n",
        "\n",
        "Zadaniem jest zaimplementowanie \"od zera\" sieci konwolucyjnej do klasyfikacji wieloklasowej na zbiorze CIFAR10. Należy zaimplementować zarówno model jak i pętle uczenia oraz ewaluacji. Twój kod powinien raportować *loss* w trakcie trenowania, testowania oraz *accuracy* na zbiorze testowym (opcjonalnie również na treningowym). Na koniec *accuracy* powinno osiągnąć wynik powyżej 75% na zbiorze testowym. Można korzystać z dowolnych mechanizmów dostępnych w PyTorchu.\n",
        "\n",
        "Porady do zadania:\n",
        "\n",
        "* Zwiększenie treningowego *batch size* może przyspieszyć uczenie, ale należy pamiętać, że, tak jak każdy hiperparametr, *batch_size* ma wpływ na proces uczenia.\n",
        "* Przy dobieraniu architektury bądź hiperparametrów nie testować dłużej niż kilka epok (około 5), tak aby mieć szybki feedback. Dopiero po dobraniu powyższych rzeczy puścić dłuższe uczenie. Warto przy okazji wizualizować krzywe uczenia (funkcji kosztu) aby zwrócić uwagę na to kiedy koszt zaczyna się \"wypłaszczać\" i czy model przypadkowo nie overfituje (patrząc na wartości kosztu na zbiorze testowym)\n",
        "* Dodatkowe (nadobowiązkowe) rzeczy do poprawienia wyniku: LR Schedule, Early Stopping."
      ]
    },
    {
      "metadata": {
        "id": "-wJzSMLVl172"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# tutaj zdefiniuj swój model, nie zapomnij o dziedziczeniu z torch.nn.Module\n"
      ]
    },
    {
      "metadata": {
        "id": "QtJ9ONeSl172"
      },
      "cell_type": "code",
      "source": [
        "# tutaj uzupełnij ładowanie danych treningowych i testowych\n",
        "\n",
        "train_dataset = CIFAR10(root='.',\n",
        "                        train=True,\n",
        "                        download=True,\n",
        "                        transform=???)\n",
        "\n",
        "test_dataset = CIFAR10(root='.',\n",
        "                       train=False,\n",
        "                       download=True,\n",
        "                       transform=???)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=???, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5000, shuffle=False, num_workers=4)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "I3nTM7Dkl172"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# tutaj zaimplementuj pętle uczenia ze wszystkimi potrzebnymi hiperparametrami\n"
      ]
    },
    {
      "metadata": {
        "id": "3Q-8vYNvl172"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie 4 ResNet (2 pkt.)\n",
        "\n",
        "Zaimplementuj sieć konwolucyjną do klasyfikacji wieloklasowej na zbiorze CIFAR10 z blokami rezydualnymi.\n",
        "```\n",
        "Standard Block:  \n",
        "x → Conv → ReLU → Conv → y  \n",
        "\n",
        "Residual Block:  \n",
        "x → Conv → BN → ReLU → Conv → BN → (+) → ReLU → y  \n",
        "                         ↑  \n",
        "                         x (identity lub projekcja)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "HzyEWPFpl172"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# tutaj zdefiniuj swój model, nie zapomnij o dziedziczeniu z torch.nn.Module"
      ]
    },
    {
      "metadata": {
        "id": "M3rHs8Iul172"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Wykorzystaj pętlę uczenia z poprzedniego zadania"
      ]
    },
    {
      "metadata": {
        "id": "T_T1KGzpl172"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie 5 Augmentacja danych (2 pkt.)\n",
        "\n",
        "Używając funkcjonalności dostępnych w [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) dodaj augmentacje danych do datasetu treningowego, zobacz o ile poprawia to wynik twoich  modeli z poprzedniech zadań. W razie wątpliwości najlepiej przeszukać internet w poszukiwaniu typowych augmentacji dla CIFAR10."
      ]
    },
    {
      "metadata": {
        "id": "QOVchYJKl173"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# load test set\n",
        "dataset = CIFAR10(root=\".\", train=True, transform=ToTensor())\n",
        "loader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
        "\n",
        "# prepare a random flip\n",
        "flip = RandomVerticalFlip(p=1)\n",
        "\n",
        "# load some data\n",
        "for x, y in loader:\n",
        "    break\n",
        "\n",
        "\n",
        "# visualise the flipping\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(30, 15))\n",
        "\n",
        "for img, label, ax in zip(x, y, axes[0]):\n",
        "    ax.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    ax.set_title(f\"{dataset.classes[label]}\", fontdict={\"size\": 30})\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "for img, label, ax in zip(x, y, axes[1]):\n",
        "    ax.imshow(np.transpose(flip(img), (1, 2, 0)))\n",
        "    ax.set_title(f\"Flipped {dataset.classes[label]}\", fontdict={\"size\": 30})\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "metadata": {
        "id": "LF9OMEUHl173"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# tutaj dodaj wybrane augumentcje danych treningowych i załaduj testowe\n",
        "\n",
        "train_dataset = CIFAR10(root='.',\n",
        "                        train=True,\n",
        "                        download=True\n",
        "                        transform=???)\n",
        "\n",
        "test_dataset = CIFAR10(root='.',\n",
        "                       train=False,\n",
        "                       download=True\n",
        "                       transform=???)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=???, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5000, shuffle=False, num_workers=4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}