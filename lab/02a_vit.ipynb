{
  "cells": [
    {
      "metadata": {
        "id": "d52880382e47912f"
      },
      "cell_type": "markdown",
      "source": [
        "## Visual Transformer"
      ],
      "id": "d52880382e47912f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72026f93-981a-4fa0-96d5-0ebc3a97208e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ml2025-26'...\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # one of the ways to check if our notebook is running inside google colab\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone  https://github.com/gmum/ml2025-26.git\n",
        "    import sys\n",
        "    sys.path.append('/content/ml2025-26/lab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/\")"
      ],
      "metadata": {
        "id": "7mK2_jkOz2gv"
      },
      "id": "7mK2_jkOz2gv",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ac288a164ef029b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "4a0d6956-6509-4b68-9dc9-4242c042eedc"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'save_experiment' from 'utils' (/content/utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3998147445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_experiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_experiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from checker import (\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'save_experiment' from 'utils' (/content/utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 16,
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from utils import save_experiment, save_checkpoint, load_experiment, visualize_images, visualize_attention\n",
        "\n",
        "from checker import (\n",
        "    expected_vit_embeddings_output,\n",
        "    expected_vit_attention_head_output, expected_vit_attention_head_probs,\n",
        "    expected_vit_multi_head_attention_output, expected_vit_multi_head_attention_probs,\n",
        "    expected_vit_faster_multi_head_attention_output, expected_vit_faster_multi_head_attention_probs\n",
        ")"
      ],
      "id": "ac288a164ef029b"
    },
    {
      "metadata": {
        "id": "6d384e264bf7bf23"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN vs ViT processing\n",
        "\n",
        "```\n",
        "CNN Processing:\n",
        "Input → Conv(3×3) → Conv(3×3) → Conv(3×3) → ... → Global Pool → FC\n",
        "        ↓           ↓           ↓\n",
        "      Local        Local        Local\n",
        "     (receptive field grows slowly)\n",
        "\n",
        "ViT Processing:  \n",
        "Input → Patches → Self-Attention → Self-Attention → ... → [CLS] Token → FC\n",
        "                     ↓               ↓\n",
        "                   Global          Global\n",
        "                (immediate full-image context)\n",
        "```"
      ],
      "id": "6d384e264bf7bf23"
    },
    {
      "metadata": {
        "id": "9cea7e1023518605"
      },
      "cell_type": "markdown",
      "source": [
        "### ViT implementation"
      ],
      "id": "9cea7e1023518605"
    },
    {
      "metadata": {
        "id": "b866eb2d91866176"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie 1: PatchEmbeddings (1 pkt.)\n"
      ],
      "id": "b866eb2d91866176"
    },
    {
      "metadata": {
        "id": "3ee6f7d9ce79c636"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 4,
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert the image into patches and then project them into a vector space.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.num_channels = config[\"num_channels\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        # Calculate the number of patches from the image size and patch size\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        # Create a projection layer to convert the image into patches\n",
        "        # The layer projects each patch into a vector of size hidden_size\n",
        "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine the patch embeddings with the class token and position embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embeddings = PatchEmbeddings(config)\n",
        "        # Create a learnable [CLS] token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
        "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
        "        # Add 1 to the sequence length for the [CLS] token\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.patch_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Expand the [CLS] token to the batch size\n",
        "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
        "        cls_tokens = self.cls_token.repeat(batch_size, 1, 1)\n",
        "        # Concatenate the [CLS] token to the beginning of the input sequence and add positional position_embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.position_embeddings\n",
        "        return self.dropout(x)"
      ],
      "id": "3ee6f7d9ce79c636"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embeddings():\n",
        "  torch.manual_seed(0)\n",
        "  embeddings = Embeddings({\n",
        "      \"image_size\": 16,\n",
        "      \"patch_size\": 4,\n",
        "      \"hidden_size\": 4,\n",
        "      \"num_channels\": 3,\n",
        "      \"hidden_dropout_prob\": 0.0,\n",
        "  })\n",
        "  input_data = torch.randn(1, 3, 16, 16)\n",
        "  result = embeddings(input_data)\n",
        "  assert torch.allclose(result, expected_vit_embeddings_output, atol=1e-3)\n",
        "  print(\"Passed\")\n",
        "\n",
        "test_embeddings()"
      ],
      "metadata": {
        "id": "UbQiuCkcJvGb"
      },
      "id": "UbQiuCkcJvGb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48bd6fa353c17bf"
      },
      "cell_type": "markdown",
      "source": [
        "### Zadanie 2: Attention (2 pkt.)\n"
      ],
      "id": "48bd6fa353c17bf"
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A single attention head.\n",
        "    This module is used in the MultiHeadAttention module.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project the input into query, key, and value\n",
        "        # The same input is used to generate the query, key, and value,\n",
        "        # so it's usually called self-attention.\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        # Calculate the attention:\n",
        "        softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        key_trans = torch.transpose(key, -1, -2)\n",
        "        attention_probs = softmax((query @ key_trans) / (self.attention_head_size)**(1/2))\n",
        "\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        attention_output = attention_probs @ value\n",
        "        return (attention_output, attention_probs)"
      ],
      "metadata": {
        "id": "qpy4cd6pNTRu"
      },
      "id": "qpy4cd6pNTRu",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention_head():\n",
        "  torch.manual_seed(0)\n",
        "  attention = AttentionHead(**{\n",
        "      \"hidden_size\": 4,\n",
        "      \"attention_head_size\": 4,\n",
        "      \"dropout\": 0.0,\n",
        "  })\n",
        "  input_data = torch.randn(1, 8, 4)\n",
        "  attention_output, attention_probs = attention(input_data)\n",
        "\n",
        "  assert torch.allclose(attention_output, expected_vit_attention_head_output, atol=1e-3)\n",
        "  assert torch.allclose(attention_probs, expected_vit_attention_head_probs, atol=1e-3)\n",
        "  print(\"Passed\")\n",
        "\n",
        "test_attention_head()"
      ],
      "metadata": {
        "id": "RCq_cBT7NVgA"
      },
      "id": "RCq_cBT7NVgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 3: MultiHeadAttention (1 pkt.)"
      ],
      "metadata": {
        "id": "Azg_B_VIQVZ1"
      },
      "id": "Azg_B_VIQVZ1"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module.\n",
        "    This module is used in the TransformerEncoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([])\n",
        "        for _ in range(self.num_attention_heads):\n",
        "            head = AttentionHead(\n",
        "                self.hidden_size,\n",
        "                self.attention_head_size,\n",
        "                config[\"attention_probs_dropout_prob\"],\n",
        "                self.qkv_bias\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the attention output for each attention head\n",
        "        attention_outputs = [head(x) for head in self.heads]\n",
        "        # Concatenate the attention outputs from each attention head\n",
        "        attention_output = torch.cat([output for output, _probs in attention_outputs], dim=-1)\n",
        "        # Project the concatenated attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
        "            return (attention_output, attention_probs)"
      ],
      "metadata": {
        "id": "4rKxqsUVQRv7"
      },
      "id": "4rKxqsUVQRv7",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_head_attention_head():\n",
        "  torch.manual_seed(0)\n",
        "  attention = MultiHeadAttention({\n",
        "      \"hidden_size\": 6,\n",
        "      \"num_attention_heads\": 2,\n",
        "      \"attention_probs_dropout_prob\": 0.0,\n",
        "      \"hidden_dropout_prob\": 0.0,\n",
        "      \"qkv_bias\": True,\n",
        "  })\n",
        "  input_data = torch.randn(1, 8, 6)\n",
        "  attention_output, attention_probs = attention(input_data, output_attentions=True)\n",
        "  assert torch.allclose(attention_output, expected_vit_multi_head_attention_output, atol=1e-3)\n",
        "  assert torch.allclose(attention_probs, expected_vit_multi_head_attention_probs, atol=1e-3)\n",
        "  print(\"Passed\")\n",
        "\n",
        "test_multi_head_attention_head()"
      ],
      "metadata": {
        "id": "b6RQlSFMQr5l"
      },
      "id": "b6RQlSFMQr5l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 4. FasterMultiHeadAttention (3 pkt.)"
      ],
      "metadata": {
        "id": "Qj78FFpYSJ1_"
      },
      "id": "Qj78FFpYSJ1_"
    },
    {
      "metadata": {
        "id": "1100b2b5ac8214cb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 7,
      "source": [
        "class FasterMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module with some optimizations.\n",
        "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a linear layer to project the query, key, and value\n",
        "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Project the query, key, and value\n",
        "        qkv = self.qkv_projection(x)\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
        "        # Split the projected query, key, and value into query, key, and value\n",
        "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        batch_size, sequence_length, _ = query.size()\n",
        "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "\n",
        "        # Calculate the attention:\n",
        "        softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        key_trans = torch.transpose(key, -1, -2)\n",
        "        attention_probs = softmax((query @ key_trans) / (self.attention_head_size)**(1/2))\n",
        "\n",
        "        attention_output = attention_probs @ value\n",
        "        # Resize the attention output\n",
        "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        # To (batch_size, sequence_length, all_head_size)\n",
        "        attention_output = attention_output.transpose(1, 2).reshape(batch_size, sequence_length, self.all_head_size)\n",
        "\n",
        "        # Project the attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            return (attention_output, attention_probs)"
      ],
      "id": "1100b2b5ac8214cb"
    },
    {
      "metadata": {
        "id": "3a03ce355359ba85"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def test_faster_multi_head_attention_head():\n",
        "  torch.manual_seed(0)\n",
        "  attention = FasterMultiHeadAttention({\n",
        "      \"hidden_size\": 6,\n",
        "      \"num_attention_heads\": 2,\n",
        "      \"attention_probs_dropout_prob\": 0.0,\n",
        "      \"hidden_dropout_prob\": 0.0,\n",
        "      \"qkv_bias\": True,\n",
        "  })\n",
        "  input_data = torch.randn(1, 8, 6)\n",
        "  attention_output, attention_probs = attention(input_data, output_attentions=True)\n",
        "  assert torch.allclose(attention_output, expected_vit_faster_multi_head_attention_output, atol=1e-3)\n",
        "  assert torch.allclose(attention_probs, expected_vit_faster_multi_head_attention_probs, atol=1e-3)\n",
        "  print(\"Passed\")\n",
        "\n",
        "test_faster_multi_head_attention_head()"
      ],
      "id": "3a03ce355359ba85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT definition"
      ],
      "metadata": {
        "id": "45bJjMU_T0Yn"
      },
      "id": "45bJjMU_T0Yn"
    },
    {
      "metadata": {
        "id": "16cff92b300e8b5a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 8,
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
        "        self.activation = nn.GELU(approximate='tanh')\n",
        "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    A single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
        "        if self.use_faster_attention:\n",
        "            self.attention = FasterMultiHeadAttention(config)\n",
        "        else:\n",
        "            self.attention = MultiHeadAttention(config)\n",
        "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "        self.mlp = MLP(config)\n",
        "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Self-attention\n",
        "        attention_output, attention_probs = \\\n",
        "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
        "        # Skip connection\n",
        "        x = x + attention_output\n",
        "        # Feed-forward network\n",
        "        mlp_output = self.mlp(self.layernorm_2(x))\n",
        "        # Skip connection\n",
        "        x = x + mlp_output\n",
        "        # Return the transformer block's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, attention_probs)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The transformer encoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Create a list of transformer blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for _ in range(config[\"num_hidden_layers\"]):\n",
        "            block = Block(config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the transformer block's output for each block\n",
        "        all_attentions = []\n",
        "        for block in self.blocks:\n",
        "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
        "            if output_attentions:\n",
        "                all_attentions.append(attention_probs)\n",
        "        # Return the encoder's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, all_attentions)\n",
        "\n",
        "class ViTForClassfication(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.encoder = Encoder(config)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "        embedding_output = self.embedding(x)\n",
        "        # Calculate the encoder's output\n",
        "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
        "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
        "        logits = self.classifier(encoder_output[:, 0, :])\n",
        "        # Return the logits and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (logits, None)\n",
        "        else:\n",
        "            return (logits, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)"
      ],
      "id": "16cff92b300e8b5a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "lW8wBxOBFayu"
      },
      "id": "lW8wBxOBFayu"
    },
    {
      "metadata": {
        "id": "d39526f310584c18"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 10,
      "source": [
        "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
        "    train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='.', train=True,\n",
        "                                            download=True, transform=train_transform)\n",
        "    if train_sample_size is not None:\n",
        "        # Randomly sample a subset of the training set\n",
        "        indices = torch.randperm(len(trainset))[:train_sample_size]\n",
        "        trainset = torch.utils.data.Subset(trainset, indices)\n",
        "\n",
        "\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='.', train=False,\n",
        "                                        download=True, transform=test_transform)\n",
        "    if test_sample_size is not None:\n",
        "        # Randomly sample a subset of the test set\n",
        "        indices = torch.randperm(len(testset))[:test_sample_size]\n",
        "        testset = torch.utils.data.Subset(testset, indices)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    return trainloader, testloader, classes"
      ],
      "id": "d39526f310584c18"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training config"
      ],
      "metadata": {
        "id": "mtZLPKogF3aA"
      },
      "id": "mtZLPKogF3aA"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    The simple trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.exp_name = exp_name\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "        \"\"\"\n",
        "        # Keep track of the losses and accuracies\n",
        "        train_losses, test_losses, accuracies = [], [], []\n",
        "        # Train the model\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(trainloader)\n",
        "            accuracy, test_loss = self.evaluate(testloader)\n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(test_loss)\n",
        "            accuracies.append(accuracy)\n",
        "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
        "                print('\\tSave checkpoint at epoch', i+1)\n",
        "                save_checkpoint(self.exp_name, self.model, i+1)\n",
        "        # Save the experiment\n",
        "        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)\n",
        "\n",
        "    def train_epoch(self, trainloader):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(trainloader):\n",
        "            # Move the batch to the device\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            images, labels = batch\n",
        "            # Zero the gradients\n",
        "            self.optimizer.zero_grad()\n",
        "            # Calculate the loss\n",
        "            loss = self.loss_fn(self.model(images)[0], labels)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "            # Update the model's parameters\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "        return total_loss / len(trainloader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, testloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(testloader):\n",
        "                # Move the batch to the device\n",
        "                batch = [t.to(self.device) for t in batch]\n",
        "                images, labels = batch\n",
        "\n",
        "                # Get predictions\n",
        "                logits, _ = self.model(images)\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.loss_fn(logits, labels)\n",
        "                total_loss += loss.item() * len(images)\n",
        "\n",
        "                # Calculate the accuracy\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += torch.sum(predictions == labels).item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        avg_loss = total_loss / len(testloader.dataset)\n",
        "        return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "W0hnSfMVHanU"
      },
      "id": "W0hnSfMVHanU",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie 5. Train ViT (3 pkt.)\n",
        "\n",
        "Na koniec *accuracy* powinno osiągnąć wynik powyżej 75% na zbiorze testowym.\n"
      ],
      "metadata": {
        "id": "xgkpo9a5T3_a"
      },
      "id": "xgkpo9a5T3_a"
    },
    {
      "metadata": {
        "id": "a75db502a3c9ef95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "8a28a0bd-fbf8-443f-9e75-b5d2e6e46f20"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.63it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 1.9317, Test loss: 1.7460, Accuracy: 0.3435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:27<00:00, 14.39it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 33.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 1.6647, Test loss: 1.5872, Accuracy: 0.4179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.65it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 33.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 1.5444, Test loss: 1.4859, Accuracy: 0.4599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.57it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 32.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train loss: 1.4493, Test loss: 1.3798, Accuracy: 0.5060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:27<00:00, 14.12it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 32.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Train loss: 1.3918, Test loss: 1.3433, Accuracy: 0.5145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.61it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 33.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Train loss: 1.3510, Test loss: 1.3087, Accuracy: 0.5251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.53it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 33.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Train loss: 1.3138, Test loss: 1.3199, Accuracy: 0.5266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:27<00:00, 14.37it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Train loss: 1.2817, Test loss: 1.2622, Accuracy: 0.5413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.64it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 32.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Train loss: 1.2545, Test loss: 1.2321, Accuracy: 0.5551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:26<00:00, 14.56it/s]\n",
            "100%|██████████| 79/79 [00:02<00:00, 33.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Train loss: 1.2282, Test loss: 1.2063, Accuracy: 0.5628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_experiment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1575220703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_every_n_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_model_every_n_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4027658837.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader, testloader, epochs, save_model_every_n_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Save the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msave_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_experiment' is not defined"
          ]
        }
      ],
      "execution_count": 13,
      "source": [
        "exp_name = 'vit-with-10-epochs'\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "save_model_every = 0\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = {\n",
        "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 48,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 32,\n",
        "    \"num_classes\": 10, # num_classes of CIFAR10\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True, # if task 4 is done\n",
        "}\n",
        "\n",
        "# These are not hard constraints, but are used to prevent misconfigurations\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "save_model_every_n_epochs = save_model_every\n",
        "# Load the CIFAR10 dataset\n",
        "trainloader, testloader, _ = prepare_data(batch_size=batch_size)\n",
        "# Create the model, optimizer, loss function and trainer\n",
        "model = ViTForClassfication(config)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)\n",
        "trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n",
        "\n"
      ],
      "id": "a75db502a3c9ef95"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Visualization"
      ],
      "metadata": {
        "id": "qIc3EUzAUR4q"
      },
      "id": "qIc3EUzAUR4q"
    },
    {
      "metadata": {
        "id": "306b4e00428f4945"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "visualize_attention(model, \"attention.png\")"
      ],
      "id": "306b4e00428f4945"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}